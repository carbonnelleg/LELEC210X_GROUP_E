{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from classification.datasets import Dataset_augmented\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS\n",
    "from classification.utils.plots import (\n",
    "    plot_decision_boundaries,\n",
    "    plot_specgram,\n",
    "    show_confusion_matrix,\n",
    ")\n",
    "from classification.utils.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_trimming.py must be run before executing this cell!\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset_augmented()\n",
    "classnames = dataset.list_classes() #['chainsaw', 'fire', 'fireworks', 'gun']\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "number_audio_files = 280\n",
    "# splitting it into training and test sets\n",
    "for i in range(4):\n",
    "    random_indexes = [np.random.choice(number_audio_files, int(number_audio_files*0.7), replace=False) for _ in range(4)]\n",
    "    test_indexes = [np.setdiff1d(np.arange(number_audio_files), random_indexes[i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"src/classification/datasets/melspectrograms/\"\n",
    "def fv_generator(Nft_num, nmel_num):\n",
    "    Nft = Nft_num\n",
    "    nmel = nmel_num\n",
    "    # threshold = 0.05 # threshold under which we discard the input\n",
    "    \n",
    "    # Computing the mel spectrogram of each audio file and saving in a folder\n",
    "    n_win_files = np.zeros(4*number_audio_files)\n",
    "    \n",
    "    for class_index in range (len(classnames)):\n",
    "        for audio_index in range(number_audio_files):\n",
    "            current_sound = dataset[classnames[class_index], audio_index]\n",
    "            current_audio = AudioUtil.open(current_sound)\n",
    "            current_audio = AudioUtil.resample(current_audio, 11025)\n",
    "            \n",
    "            # we will split the audio into 1 second window, and compute the mel spectrogram of each clip\n",
    "            n_win = (len(current_audio[0]) // 11025) + 1\n",
    "            n_win_files[class_index * number_audio_files + audio_index] = int(n_win)\n",
    "            for window in range(n_win):\n",
    "                sub_aud = (current_audio[0][window * 11025 :], current_audio[1])\n",
    "                sub_aud = AudioUtil.pad_trunc(sub_aud, 950)\n",
    "                sgram = AudioUtil.melspectrogram(sub_aud, Nmel=nmel, Nft=Nft)\n",
    "                ncol = int(11025 / Nft)\n",
    "                sgram = sgram[:, :ncol]\n",
    "                fv = sgram.reshape(-1)\n",
    "                # saving the mel spectrogram in .npy format\n",
    "                np.save(folder_path + classnames[class_index] + str(audio_index) + \"_\" + str(window) + \".npy\", fv)\n",
    "                \n",
    "    fv_len = len(fv)\n",
    "    return fv_len, n_win_files\n",
    "    \n",
    "    \n",
    "def pca_generator(training_indexes, pca_comp):\n",
    "    # Creating all the feature vectors to compute the PCA\n",
    "    total_number_window_training = np.sum(n_win_files[training_indexes[0]]) + np.sum(n_win_files[training_indexes[1]]) + np.sum(n_win_files[training_indexes[2]]) + np.sum(n_win_files[training_indexes[3]])\n",
    "    X_train = np.zeros((int(total_number_window_training), int(fv_len)))\n",
    "    y_train = np.zeros(int(total_number_window_training))\n",
    "    \n",
    "    # we will use the indexes to load the mel spectrograms and compute the PCA\n",
    "    index = 0\n",
    "    for class_index in range (len(classnames)):\n",
    "        for audio_index in training_indexes[class_index]:\n",
    "            for window in range(int(n_win_files[class_index * number_audio_files + audio_index])):\n",
    "                X_train[index, :] = np.load(folder_path + classnames[class_index] + str(audio_index) + \"_\" + str(window) + \".npy\")\n",
    "                y_train[index] = class_index\n",
    "                index += 1\n",
    "                \n",
    "    # normalisation before PCA since the amplitude is not a valuable information\n",
    "    X_train_pca = X_train / np.linalg.norm(X_train, axis=0)\n",
    "    \n",
    "    # PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=pca_comp)\n",
    "    pca.fit(X_train_pca)\n",
    "    \n",
    "    return pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_fv(class_index, audio_indexes):\n",
    "    \"\"\"\n",
    "    This function takes a class index and a list of audio indexes and returns a matrix of size (len(audio_indexes), n_win_files, fv_len)\n",
    "    containing the mel spectrogram of each audio file. It also returns a list of labels (which is the same for all the windows of the same audio file)\n",
    "    \"\"\"\n",
    "    X = np.array([np.zeros((int(n_win_files[idx + class_index * number_audio_files]), fv_len)) for idx in audio_indexes], dtype=object)\n",
    "    label = np.array([[classnames[class_index]] * int(n_win_files[idx + class_index * number_audio_files]) for idx in audio_indexes],dtype=object)\n",
    "    for i, audio_index in enumerate(audio_indexes):\n",
    "        for window in range(int(n_win_files[audio_index + class_index * number_audio_files])):\n",
    "            X[i][window, :] = np.load(folder_path + classnames[class_index] + str(audio_index) + \"_\" + str(window) + \".npy\")\n",
    "            label[i][window] = classnames[class_index]\n",
    "    return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def manual_kfold_random(index_list, n_splits, seed=None):\n",
    "    \"\"\"\n",
    "    Génère des indices pour un k-fold manuel avec répartition aléatoire.\n",
    "\n",
    "    :param index_list: Liste ou array NumPy des indices totaux\n",
    "    :param n_splits: Nombre de partitions (doit être inférieur à la taille de la liste)\n",
    "    :param seed: Optionnel, permet de fixer la graine pour la reproductibilité\n",
    "    :return: Liste de tuples (learning_set, validation_set)\n",
    "    \"\"\"\n",
    "    assert n_splits < len(index_list), \"n_splits doit être inférieur au nombre total d'indices.\"\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)  # Fixe la graine pour des résultats reproductibles\n",
    "\n",
    "    index_list = list(index_list)  # Conversion en liste au cas où c'est un array NumPy\n",
    "    random.shuffle(index_list)  # Mélange les index aléatoirement\n",
    "\n",
    "    fold_size = len(index_list) // n_splits\n",
    "    folds = [index_list[i * fold_size:(i + 1) * fold_size] for i in range(n_splits)]\n",
    "    \n",
    "    if len(index_list) % n_splits != 0:  # Gestion des restes\n",
    "        folds[-1] = list(folds[-1]) + index_list[n_splits * fold_size:]  # Conversion et ajout correct\n",
    "\n",
    "    split_indices = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        validation_set = folds[i]\n",
    "        learning_set = [idx for j, fold in enumerate(folds) if j != i for idx in fold]\n",
    "        split_indices.append((learning_set, validation_set))\n",
    "    \n",
    "    return split_indices\n",
    "\n",
    "\n",
    "def predict_all_probabilities(clf, X, my_fv_len = 400, my_nmel = 20):\n",
    "    \"\"\"\n",
    "    Prédit les probabilités pour tous les feature vectors dans X.\n",
    "\n",
    "    :param clf: Classifieur entraîné avec une méthode `.predict_proba()`\n",
    "    :param X: Array de shape (len(classnames), variable, n_win, fv_len)\n",
    "    :return: Liste des probabilités prédites, réorganisées par classes, échantillons et fenêtres\n",
    "    \"\"\"\n",
    "    len_classes = X.shape[0]  # Nombre de classes\n",
    "    all_features = []\n",
    "    indices = []  # Pour garder la trace des indices originaux\n",
    "\n",
    "    # 1. Extraction des features sous forme de liste\n",
    "    for class_idx in range(len_classes):\n",
    "        for sample_idx, feature_matrix in enumerate(X[class_idx]):  # X[class_idx] est de shape (variable, n_win, fv_len)\n",
    "            num_windows = feature_matrix.shape[0]  # Nombre de fenêtres pour cet échantillon\n",
    "            all_features.append(feature_matrix)  # Stocke les features\n",
    "            indices.extend([(class_idx, sample_idx)] * num_windows)  # Associe chaque fenêtre à son (class, sample)\n",
    "\n",
    "    # 2. Conversion en un array unique pour accélérer la prédiction\n",
    "    X_flattened = np.vstack(all_features)  # Shape: (total_windows, fv_len)\n",
    "    X_flattened = X_flattened / np.linalg.norm(X_flattened, axis=0)  # Normal\n",
    "    \n",
    "    # if model is not a CNN, we need to reduce the dimensions\n",
    "    if clf.__class__.__name__ != 'Sequential':\n",
    "        X_flattened = pca.transform(X_flattened)\n",
    "\n",
    "    # 3. Prédiction des probabilités en batch\n",
    "    # if the model is a CNN, we need to reshape the input and to use the predict method\n",
    "    if clf.__class__.__name__ == 'Sequential':\n",
    "        new_shape = (X_flattened.shape[0], my_nmel, my_fv_len // my_nmel, 1)\n",
    "        X_flattened = X_flattened.reshape(new_shape)\n",
    "        probas_flattened = clf.predict(X_flattened)  # Sortie: (total_windows, n_classes)\n",
    "    else:\n",
    "        probas_flattened = clf.predict_proba(X_flattened)  # Sortie: (total_windows, n_classes)\n",
    "\n",
    "    # 4. Réorganisation des probabilités\n",
    "    probabilities = [[] for _ in range(len_classes)]\n",
    "    for (class_idx, sample_idx), proba in zip(indices, probas_flattened):\n",
    "        if len(probabilities[class_idx]) <= sample_idx:\n",
    "            probabilities[class_idx].append([])  # Assure que la liste existe\n",
    "        probabilities[class_idx][sample_idx].append(proba)  # Ajoute la probabilité pour chaque fenêtre\n",
    "\n",
    "    return probabilities  # Liste [classes][samples][windows][probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_naive(probs):\n",
    "    \"\"\"Règle naïve : Choisir la classe avec la probabilité la plus élevée.\"\"\"\n",
    "    return classnames[np.argmax(probs) % len(classnames)]\n",
    "\n",
    "def decision_majority(probs):\n",
    "    \"\"\"Règle du vote majoritaire : Choisir la classe avec le plus grand nombre de votes.\"\"\"\n",
    "    votes = np.argmax(probs, axis=1)\n",
    "    count = np.bincount(votes)\n",
    "    max_indices = np.where(count == count.max())[0]\n",
    "    return classnames[max_indices[0]]  # En cas d'égalité, on prend la première classe\n",
    "\n",
    "def decision_weighted(probs):\n",
    "    \"\"\"Règle du vote pondéré : Choisir la classe avec la somme maximale des probabilités.\"\"\"\n",
    "    sum_probs = np.sum(probs, axis=0)\n",
    "    return classnames[np.argmax(sum_probs)]\n",
    "\n",
    "def decision_maxlikelihood(probs):\n",
    "    \"\"\"Règle du maximum de vraisemblance : Choisir la classe avec le produit maximal des probabilités.\"\"\"\n",
    "    prod_probs = np.prod(probs, axis=0)\n",
    "    return classnames[np.argmax(prod_probs)]\n",
    "\n",
    "# Application de la règle de décision à chaque échantillon d'un audio\n",
    "def apply_decision_rules(probabilities, decision_method):\n",
    "    \"\"\"\n",
    "    Applique une règle de décision sur toutes les fenêtres d'un échantillon.\n",
    "\n",
    "    :param probabilities: Probabilités des fenêtres pour un échantillon (n_win, n_classes)\n",
    "    :param decision_method: Méthode de décision à utiliser parmi 'naive', 'majority', 'weighted', 'maxlikelihood' [0, 1, 2, 3]\n",
    "    :return: Classe prédite pour l'échantillon\n",
    "    \"\"\"\n",
    "    if decision_method == 0:\n",
    "        return decision_naive(probabilities)\n",
    "    elif decision_method == 1:\n",
    "        return decision_majority(probabilities)\n",
    "    elif decision_method == 2:\n",
    "        return decision_weighted(probabilities)\n",
    "    elif decision_method == 3:\n",
    "        return decision_maxlikelihood(probabilities)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown decision method: \" + decision_method)\n",
    "    \n",
    "dict_decision_methods = {\n",
    "    0: \"Naive\",\n",
    "    1: \"Majority\",\n",
    "    2: \"Weighted\",\n",
    "    3: \"MaxLikelihood\"\n",
    "}\n",
    "\n",
    "\n",
    "def predict_for_all_samples(probabilities, decision_method):\n",
    "    \"\"\"\n",
    "    Applique une règle de décision sur tous les échantillons (classes, samples) du dataset.\n",
    "\n",
    "    :param probabilities: Liste des probabilités pour chaque fenêtre de chaque échantillon.\n",
    "    :param decision_method: Méthode de décision ('naive', 'majority', 'weighted', 'maxlikelihood')\n",
    "    :return: Liste des classes prédites pour chaque échantillon\n",
    "    \"\"\"\n",
    "    n_classes = len(probabilities)\n",
    "    n_samples = len(probabilities[0])\n",
    "    predictions = []\n",
    "\n",
    "    for class_idx in range(n_classes):\n",
    "        class_predictions = []\n",
    "        for sample_idx in range(n_samples):\n",
    "            # Probabilités des fenêtres pour un échantillon donné\n",
    "            probs = probabilities[class_idx][sample_idx]\n",
    "            # Applique la règle de décision sur l'échantillon\n",
    "            predicted_class = apply_decision_rules(probs, decision_method)\n",
    "            class_predictions.append(predicted_class)\n",
    "        predictions.append(class_predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(predictions, y_val):\n",
    "    \"\"\"\n",
    "    Calcule l'accuracy en comparant les prédictions avec les labels réels.\n",
    "\n",
    "    :param predictions: Liste des prédictions pour chaque échantillon et chaque classe\n",
    "                        (n_classes, n_samples) - chaque élément est la classe prédite pour un échantillon.\n",
    "    :param y_val: Array des labels réels pour chaque échantillon (n_classes, n_samples)\n",
    "                  Chaque élément est la classe réelle de cet échantillon.\n",
    "    :return: L'accuracy du modèle en pourcentage\n",
    "    \"\"\"\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    n_classes = len(predictions)\n",
    "\n",
    "    for class_idx in range(n_classes):\n",
    "        for sample_idx in range(len(predictions[class_idx])):\n",
    "            # Comparer la prédiction avec le label réel\n",
    "            predicted_class = predictions[class_idx][sample_idx]\n",
    "            true_class = y_val[class_idx][sample_idx][0]\n",
    "            \n",
    "            # Si la prédiction est correcte, on incrémente le compteur\n",
    "            if predicted_class == true_class:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "    # Calculer l'accuracy\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    H, W, C = input_shape  # Hauteur, Largeur, Canaux\n",
    "\n",
    "    # Définition des tailles de kernels basées sur la taille de l'image\n",
    "    k1 = (max(1, H // 6), max(1, W // 6))  # Premier kernel (minimum 1x1)\n",
    "    k2 = (max(1, H // 7), max(1, W // 7))  # Deuxième kernel\n",
    "    k3 = (max(1, H // 8), max(1, W // 8))  # Troisième kernel\n",
    "    k4 = (max(1, H // 9), max(1, W // 9))  # Quatrième kernel\n",
    "    k5 = (max(1, H // 10), max(1, W // 10)) # Cinquième kernel\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    # L1: Convolution + ReLU + MaxPooling\n",
    "    model.add(layers.Conv2D(24, k1, activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # L2: Convolution + ReLU + Padding + MaxPooling\n",
    "    model.add(layers.Conv2D(48, k2, activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # L3: Convolution + ReLU + Padding (No MaxPooling)\n",
    "    model.add(layers.Conv2D(48, k3, activation='relu', padding='same'))\n",
    "\n",
    "    # L4: Convolution + ReLU + Padding (No MaxPooling)\n",
    "    model.add(layers.Conv2D(60, k4, activation='relu', padding='same'))\n",
    "\n",
    "    # L5: Convolution + ReLU + Padding (No MaxPooling)\n",
    "    model.add(layers.Conv2D(72, k5, activation='relu', padding='same'))\n",
    "\n",
    "    # Flatten the output for the dense layers\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Déterminer dynamiquement la taille du premier dense\n",
    "    final_dim = (H // 4) * (W // 4) * 72  # Taille après convolutions et max-pooling\n",
    "    dense_units = max(64, final_dim // 10)  # Éviter un nombre trop petit\n",
    "    \n",
    "    # L6: Fully Connected + ReLU + Dropout\n",
    "    model.add(layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # L7: Output Layer with Softmax\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will try training in a different way now, basically we want that the models fits to melspectrogram of the audio files of a subset of the training set\n",
    "# but for the validation, we want to evaluate it on a sequence of melspectrogram from the complementary subset of the training set\n",
    "\n",
    "# Model training\n",
    "# we will test three models, CNN, SVM and Random Forest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# TO DO : implement the CNN model\n",
    "\n",
    "# 5-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "n_splits = 5\n",
    "seed = 42\n",
    "\n",
    "kfold_indexes = [[], [], [], []]\n",
    "for class_index in range (len(classnames)):\n",
    "    kfold_indexes[class_index] = manual_kfold_random(random_indexes[class_index], n_splits, seed=seed)\n",
    "\n",
    "# hyperparameters of the spectrogram\n",
    "Nft_list = [256, 512, 1024]\n",
    "nmel_list = [15, 20, 30]\n",
    "pca_comp_list = [0.25, 0.5, 0.75, 1]\n",
    "\n",
    "# Random Forest\n",
    "n_estimators = [100]\n",
    "max_depth = [20]\n",
    "min_samples_split = [5]\n",
    "\n",
    "results_RF = pd.DataFrame(columns=[\"model\", \"n_estimators\", \"max_depth\", \"min_samples_split\", \"accuracy\", \"decision_method\", \"Nft\", \"nmel\", \"pca_comp\"])\n",
    "\n",
    "for Nft in Nft_list:\n",
    "    for nmel in nmel_list:\n",
    "        for pca_comp in pca_comp_list:\n",
    "            fv_len, n_win_files = fv_generator(Nft, nmel)\n",
    "            pca = pca_generator(random_indexes, int(pca_comp*fv_len))\n",
    "            for n in n_estimators:\n",
    "                for d in max_depth:\n",
    "                    for s in min_samples_split:\n",
    "                        clf = RandomForestClassifier(n_estimators=n, max_depth=d, min_samples_split=s)\n",
    "                        accuracies = np.zeros(len(classnames))\n",
    "                        \n",
    "                        # creating the indexes for each split\n",
    "                        kfold_indexes = [[], [], [], []]\n",
    "                        for class_index in range (len(classnames)):\n",
    "                            kfold_indexes[class_index] = manual_kfold_random(random_indexes[class_index], n_splits, seed=seed)\n",
    "                            \n",
    "                        # kfold cross validation\n",
    "                        for split in range(n_splits):\n",
    "                            X_train = np.empty(len(classnames), dtype=object)\n",
    "                            y_train = np.empty(len(classnames), dtype=object)\n",
    "                            X_val = np.empty(len(classnames), dtype=object)\n",
    "                            y_val = np.empty(len(classnames), dtype=object)\n",
    "                            for class_index in range (len(classnames)):\n",
    "                                X_train[class_index], y_train[class_index] = audio_to_fv(class_index, kfold_indexes[class_index][split][0])\n",
    "                                X_val[class_index], y_val[class_index] = audio_to_fv(class_index, kfold_indexes[class_index][split][1])\n",
    "                                \n",
    "                            # Training\n",
    "                            X_train = np.vstack([sample for class_samples in X_train for sample in class_samples])\n",
    "                            y_train = np.hstack([np.repeat(label, len(samples)) for samples, labels in zip(X_train, y_train) for label in labels])\n",
    "                            y_train = y_train[::fv_len]\n",
    "                            X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "                            X_train = pca.transform(X_train)\n",
    "                            clf.fit(X_train, y_train)\n",
    "                            \n",
    "                            # Validation\n",
    "                            probas = predict_all_probabilities(clf, X_val)\n",
    "                            for decision_index in range(4):\n",
    "                                accuracies[decision_index] += compute_accuracy(predict_for_all_samples(probas,decision_index), y_val)\n",
    "                                \n",
    "                        # averaging the accuracies\n",
    "                        accuracies /= n_splits\n",
    "                        for decision_index in range(4):\n",
    "                            results_RF = results_RF._append({\"model\": \"Random Forest\", \"n_estimators\": n, \"max_depth\": d, \"min_samples_split\": s, \"accuracy\": accuracies[decision_index], \"decision_method\": dict_decision_methods[decision_index], \"Nft\": Nft, \"nmel\": nmel, \"pca_comp\": pca_comp}, ignore_index=True)\n",
    "                    print(results_RF.loc[results_RF[\"accuracy\"].idxmax()])\n",
    "\n",
    "# Save the results in .csv\n",
    "results_RF.to_csv(\"results_RF.csv\")\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "# 5-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "n_splits = 5\n",
    "seed = 42\n",
    "\n",
    "# hyperparameters of the spectrogram\n",
    "Nft_list = [128, 256, 512, 1024, 2048]\n",
    "nmel_list = [10, 15, 20, 25, 30]\n",
    "\n",
    "# CNN hyperparameters\n",
    "epochs_list = [10]\n",
    "batch_size_list = [32]\n",
    "\n",
    "results_CNN = pd.DataFrame(columns=[\"model\", \"epochs\", \"batch_size\", \"accuracy\", \"decision_method\", \"Nft\", \"nmel\"])\n",
    "\n",
    "for Nft in Nft_list:\n",
    "    for nmel in nmel_list:\n",
    "        fv_len, n_win_files = fv_generator(Nft, nmel)\n",
    "        for e in epochs_list:\n",
    "            for b in batch_size_list:\n",
    "                for decision_index in range(3,4):\n",
    "                    accuracies = np.zeros(len(classnames))\n",
    "                    \n",
    "                    # creating the indexes for each split\n",
    "                    kfold_indexes = [[], [], [], []]\n",
    "                    for class_index in range (len(classnames)):\n",
    "                        kfold_indexes[class_index] = manual_kfold_random(random_indexes[class_index], n_splits, seed=seed)\n",
    "                        \n",
    "                    # kfold cross validation\n",
    "                    for split in range(n_splits):\n",
    "                        X_train = np.empty(len(classnames), dtype=object)\n",
    "                        y_train = np.empty(len(classnames), dtype=object)\n",
    "                        X_val = np.empty(len(classnames), dtype=object)\n",
    "                        y_val = np.empty(len(classnames), dtype=object)\n",
    "                        for class_index in range (len(classnames)):\n",
    "                            X_train[class_index], y_train[class_index] = audio_to_fv(class_index, kfold_indexes[class_index][split][0])\n",
    "                            X_val[class_index], y_val[class_index] = audio_to_fv(class_index, kfold_indexes[class_index][split][1])\n",
    "                            \n",
    "                        # Training\n",
    "                        X_train = np.vstack([sample for class_samples in X_train for sample in class_samples])\n",
    "                        y_train = np.hstack([np.repeat(label, len(samples)) for samples, labels in zip(X_train, y_train) for label in labels])\n",
    "                        y_train = y_train[::fv_len]\n",
    "                        X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "                        \n",
    "                        # Reshape the data for the CNN\n",
    "                        new_shape = (X_train.shape[0], nmel, fv_len // nmel, 1)\n",
    "                        X_train = X_train.reshape(new_shape)\n",
    "                        \n",
    "                        # Create the CNN model\n",
    "                        model = create_cnn((nmel, fv_len // nmel, 1), len(classnames))\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                        \n",
    "                        # One-hot encoding of the labels\n",
    "                        y_train = pd.get_dummies(y_train).values\n",
    "                        \n",
    "                        # Training\n",
    "                        model.fit(X_train, y_train, epochs=e, batch_size=b)\n",
    "                        \n",
    "                        # Validation\n",
    "                        probas = predict_all_probabilities(model, X_val, fv_len, nmel)\n",
    "                        accuracies[decision_index] += compute_accuracy(predict_for_all_samples(probas,decision_index), y_val)\n",
    "                        \n",
    "                    # averaging the accuracies\n",
    "                    accuracies /= n_splits\n",
    "                    for decision_index in range(4):\n",
    "                        results_CNN = results_CNN._append({\"model\": \"CNN\", \"epochs\": e, \"batch_size\": b, \"accuracy\": accuracies[decision_index], \"decision_method\": dict_decision_methods[decision_index], \"Nft\": Nft, \"nmel\": nmel}, ignore_index=True)\n",
    "                print(results_CNN.loc[results_CNN[\"accuracy\"].idxmax()])\n",
    "                    \n",
    "# Save the results in .csv\n",
    "results_CNN.to_csv(\"results_CNN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "# 5-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "n_splits = 5\n",
    "seed = 42\n",
    "\n",
    "# hyperparameters of the spectrogram\n",
    "Nft_list = [512]\n",
    "nmel_list = [20]\n",
    "\n",
    "# SVM hyperparameters\n",
    "C_list = [1]\n",
    "kernel_list = ['rbf']\n",
    "gamma_list = ['scale']\n",
    "\n",
    "results_SVM = pd.DataFrame(columns=[\"model\", \"C\", \"kernel\", \"gamma\", \"accuracy\", \"decision_method\", \"Nft\", \"nmel\"])\n",
    "\n",
    "for Nft in Nft_list:\n",
    "    for nmel in nmel_list:\n",
    "        fv_len, n_win_files = fv_generator(Nft, nmel)\n",
    "        for c in C_list:\n",
    "            for k in kernel_list:\n",
    "                for g in gamma_list:\n",
    "                    clf = SVC(C=c, kernel=k, gamma=g)\n",
    "                    accuracies = np.zeros(len(classnames))\n",
    "                    \n",
    "                    # creating the indexes for each split\n",
    "                    kfold_indexes = [[], [], [], []]\n",
    "                    for class_index in range (len(classnames)):\n",
    "                        kfold_indexes[class_index] = manual_kfold_random(random_indexes[class_index], n_splits, seed=seed)\n",
    "                        \n",
    "                    # kfold cross validation\n",
    "                    for split in range(n_splits):\n",
    "                        X_train = np.empty(len(classnames), dtype=object)\n",
    "                        y_train = np.empty(len(classnames), dtype=object)\n",
    "                        X_val = np.empty(len(classnames), dtype=object)\n",
    "                        y_val = np.empty(len(classnames), dtype=object)\n",
    "                        for class_index in range (len(classnames)):\n",
    "                            X_train[class_index], y_train[class_index] = audio_to_fv(class_index, kfold_indexes[class_index][split][0])\n",
    "                            X_val[class_index], y_val[class_index] = audio_to_fv(class_index, kfold_indexes[class_index][split][1])\n",
    "                            \n",
    "                        # Training\n",
    "                        X_train = np.vstack([sample for class_samples in X_train for sample in class_samples])\n",
    "                        y_train = np.hstack([np.repeat(label, len(samples)) for samples, labels in zip(X_train, y_train) for label in labels])\n",
    "                        y_train = y_train[::fv_len]\n",
    "                        X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "                        X_train = pca.transform(X_train)\n",
    "                        clf.fit(X_train, y_train)\n",
    "                        \n",
    "                        # Validation\n",
    "                        probas = predict_all_probabilities(clf, X_val)\n",
    "                        for decision_index in range(4):\n",
    "                            accuracies[decision_index] += compute_accuracy(predict_for_all_samples(probas,decision_index), y_val)\n",
    "                        \n",
    "                    # averaging the accuracies\n",
    "                    accuracies /= n_splits\n",
    "                    for decision_index in range(4):\n",
    "                        results_SVM = results_SVM._append({\"model\": \"SVM\", \"C\": c, \"kernel\": k, \"gamma\": g, \"accuracy\": accuracies[decision_index], \"decision_method\": dict_decision_methods[decision_index], \"Nft\":\n",
    "Nft, \"nmel\": nmel}, ignore_index=True)\n",
    "                        \n",
    "                    print(results_SVM.loc[results_SVM[\"accuracy\"].idxmax()])\n",
    "                    \n",
    "# Save the results in .csv\n",
    "results_SVM.to_csv(\"results_SVM.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicky\\Documents\\LELEC210X_GROUP_E\\.venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.4495 - loss: 1.2028\n",
      "Epoch 2/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6396 - loss: 0.8704\n",
      "Epoch 3/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6870 - loss: 0.7938\n",
      "Epoch 4/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7283 - loss: 0.7235\n",
      "Epoch 5/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7590 - loss: 0.6524\n",
      "Epoch 6/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7644 - loss: 0.6029\n",
      "Epoch 7/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8004 - loss: 0.5271\n",
      "Epoch 8/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7892 - loss: 0.5632\n",
      "Epoch 9/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8364 - loss: 0.4459\n",
      "Epoch 10/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8322 - loss: 0.4508\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=CNN_model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39me, batch_size\u001b[38;5;241m=\u001b[39mb)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# saving the model in a format that we can use later\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCNN_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\LELEC210X_GROUP_E\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Documents\\LELEC210X_GROUP_E\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:114\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[0;32m    112\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[0;32m    113\u001b[0m     )\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filepath extension for saving. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=CNN_model."
     ]
    }
   ],
   "source": [
    "# For now we will assume the best model is CNN with epochs = 10, batch_size = 32, Nft = 512, nmel = 20\n",
    "# we will train the model on the whole training set and evaluate it on the test set\n",
    "# hyperparameters of the spectrogram\n",
    "Nft = 512\n",
    "nmel = 20\n",
    "fv_len, n_win_files = fv_generator(Nft, nmel)\n",
    "e = 10\n",
    "b = 32\n",
    "decision_index = 3\n",
    "\n",
    "# fitting on the whole training set\n",
    "X_train = np.empty(len(classnames), dtype=object)\n",
    "y_train = np.empty(len(classnames), dtype=object)\n",
    "for class_index in range (len(classnames)):\n",
    "    X_train[class_index], y_train[class_index] = audio_to_fv(class_index, random_indexes[class_index])\n",
    "X_train = np.vstack([sample for class_samples in X_train for sample in class_samples])\n",
    "y_train = np.hstack([np.repeat(label, len(samples)) for samples, labels in zip(X_train, y_train) for label in labels])\n",
    "y_train = y_train[::fv_len]\n",
    "X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "\n",
    "# Reshape the data for the CNN\n",
    "new_shape = (X_train.shape[0], nmel, fv_len // nmel, 1)\n",
    "X_train = X_train.reshape(new_shape)\n",
    "\n",
    "# Create the CNN model\n",
    "model = create_cnn((nmel, fv_len // nmel, 1), len(classnames))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# One-hot encoding of the labels\n",
    "y_train = pd.get_dummies(y_train).values\n",
    "\n",
    "# Training\n",
    "model.fit(X_train, y_train, epochs=e, batch_size=b)\n",
    "\n",
    "# saving the model in a format that we can use later\n",
    "model.save(\"CNN_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set evaluation\n",
    "# for now we will just use random forest with basic parameters\n",
    "\n",
    "# hyperparameters of the spectrogram\n",
    "Nft = 512\n",
    "nmel = 20\n",
    "\n",
    "# other hyperparameters\n",
    "pca_comp = 0.5\n",
    "best_decision = 3\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=5)\n",
    "fv_len, n_win_files = fv_generator(Nft, nmel)\n",
    "pca = pca_generator(random_indexes, int(pca_comp*fv_len))\n",
    "\n",
    "# Training\n",
    "X_train = np.empty(len(classnames), dtype=object)\n",
    "y_train = np.empty(len(classnames), dtype=object)\n",
    "for class_index in range (len(classnames)):\n",
    "    X_train[class_index], y_train[class_index] = audio_to_fv(class_index, random_indexes[class_index])\n",
    "X_train = np.vstack([sample for class_samples in X_train for sample in class_samples])\n",
    "y_train = np.hstack([np.repeat(label, len(samples)) for samples, labels in zip(X_train, y_train) for label in labels])\n",
    "y_train = y_train[::fv_len]\n",
    "X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "X_train = pca.transform(X_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Testing\n",
    "X_test = np.empty(len(classnames), dtype=object)\n",
    "y_test = np.empty(len(classnames), dtype=object)\n",
    "for class_index in range (len(classnames)):\n",
    "    X_test[class_index], y_test[class_index] = audio_to_fv(class_index, test_indexes[class_index])\n",
    "X_test = np.vstack([sample for class_samples in X_test for sample in class_samples])\n",
    "y_test = np.hstack([np.repeat(label, len(samples)) for samples, labels in zip(X_test, y_test) for label in labels])\n",
    "y_test = y_test[::fv_len]\n",
    "X_test = X_test/np.linalg.norm(X_test, axis=0)\n",
    "X_test = pca.transform(X_test)\n",
    "probas = predict_all_probabilities(model, X_test)\n",
    "predictions = predict_for_all_samples(probas, best_decision)\n",
    "accuracy = compute_accuracy(predictions, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD Model training\n",
    "# we will test three models, CNN, SVM and Random Forest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# TO DO : implement the CNN model\n",
    "\n",
    "# 5-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Random Forest\n",
    "n_estimators = [10, 50, 100, 200]\n",
    "max_depth = [5, 10, 20, 50, 100]\n",
    "min_samples_split = [2, 5, 10, 15]\n",
    "\n",
    "results_RF = pd.DataFrame(columns=[\"model\", \"n_estimators\", \"max_depth\", \"min_samples_split\", \"accuracy\"])\n",
    "\n",
    "for n in n_estimators:\n",
    "    for d in max_depth:\n",
    "        for s in min_samples_split:\n",
    "            clf = RandomForestClassifier(n_estimators=n, max_depth=d, min_samples_split=s)\n",
    "            acc = 0\n",
    "            for train_index, test_index in kf.split(X_train):\n",
    "                X_train_kf, X_test_kf = X_train[train_index], X_train[test_index]\n",
    "                y_train_kf, y_test_kf = y_train[train_index], y_train[test_index]\n",
    "                clf.fit(X_train_kf, y_train_kf)\n",
    "                acc += accuracy(y_test_kf, clf.predict(X_test_kf))\n",
    "            acc /= 5\n",
    "            results_RF = results_RF._append({\"model\": \"Random Forest\", \"n_estimators\": n, \"max_depth\": d, \"min_samples_split\": s, \"accuracy\": acc}, ignore_index=True)\n",
    "        print(results_RF.loc[results_RF[\"accuracy\"].idxmax()])\n",
    "        \n",
    "# save the results in a .csv file\n",
    "results_RF.to_csv(\"results_RF.csv\")\n",
    "\n",
    "\n",
    "# SVM\n",
    "kernel = [\"rbf\"]\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gamma = [\"scale\", \"auto\", 0.1, 0.5, 1, 5, 10]\n",
    "\n",
    "# the results will be saved in a pandas dataframe\n",
    "results_SVM = pd.DataFrame(columns=[\"model\", \"kernel\", \"C\", \"gamma\", \"accuracy\"])\n",
    "\n",
    "for k in kernel:\n",
    "    for c in C:\n",
    "        for g in gamma:\n",
    "            clf = SVC(kernel=k, C=c, gamma=g)\n",
    "            acc = 0\n",
    "            for train_index, test_index in kf.split(X_train):\n",
    "                X_train_kf, X_test_kf = X_train[train_index], X_train[test_index]\n",
    "                y_train_kf, y_test_kf = y_train[train_index], y_train[test_index]\n",
    "                clf.fit(X_train_kf, y_train_kf)\n",
    "                acc += accuracy(y_test_kf, clf.predict(X_test_kf))\n",
    "            acc /= 5\n",
    "            results_SVM = results_SVM._append({\"model\": \"SVM\", \"kernel\": k, \"C\": c, \"gamma\": g, \"accuracy\": acc}, ignore_index=True)\n",
    "        print(results_SVM.loc[results_SVM[\"accuracy\"].idxmax()])\n",
    "\n",
    "# save the results in a .csv file\n",
    "results_SVM.to_csv(\"results_SVM.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
